{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtAE6JaVO2NLwAeqd3QS7o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/58191554/PointNet-Project/blob/main/modelassert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS7B7VwzLtL3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### T-Net\n",
        "The T-Net module is a type of Spatial Transformer Network (STN) that learns a kxk transformation matrix for a given point cloud, which is then used to transform the point cloud to a canonical pose. It consists of two parts: a convolutional network and a fully connected network. The convolutional network maps the input point cloud to a feature space, consisting of a series of convolutional layers with batch normalization and ReLU activation. The fully connected network takes the feature space and learns the transformation matrix, consisting of fully connected layers with batch normalization and ReLU activation. Finally, the T-Net applies the transformation matrix to the input point cloud to transform it to a canonical pose.\n",
        "![T-net](https://github.com/58191554/PointNet-Project/blob/main/img/T-net_pipeline.drawio.png?raw=true)"
      ],
      "metadata": {
        "id": "K_4kT78yVocy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tnet(nn.Module):\n",
        "    \"\"\"\n",
        "    T-Net is a type of spatial transformer network (STN) that learns a kxk transformation matrix\n",
        "    for a given point cloud. The matrix is then used to transform the point cloud to a canonical\n",
        "    pose. It consists of two parts: a convolutional network and a fully connected network.\n",
        "    The convolutional network maps the input point cloud to a feature space and the fully connected\n",
        "    network learns the transformation matrix from the feature space.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_sizes_conv=[64, 128, 1024], hidden_sizes_fc=[512, 256], k=3):\n",
        "        super().__init__()\n",
        "        self.k=k\n",
        "        self.hidden_sizes_conv=hidden_sizes_conv\n",
        "        self.hidden_sizes_fc=hidden_sizes_fc\n",
        "        \n",
        "        self.conv = self._build_conv()\n",
        "        self.fc = self._build_fc()\n",
        "  \n",
        "    def _build_conv(self):\n",
        "        ########################################################################\n",
        "        # TODO: Builds the convolutional network that maps the input point cloud \n",
        "        # to a feature space. The hidden dimension is hidden_sizes_conv\n",
        "        #  \n",
        "        # Hint: consisting of a series of convolutional layers with batch \n",
        "        # normalization and ReLU activation.\n",
        "        #   The convolution layers is in following structure:\n",
        "        #   [conv1d]-> [Batch Norm Layer] -> [ReLU]-> [conv1d]-> ...\n",
        "        ########################################################################\n",
        "        layers = []\n",
        "        prev_size = self.k\n",
        "        for layer_id, size in enumerate(self.hidden_sizes_conv):\n",
        "            bn = nn.BatchNorm1d(size)\n",
        "            conv = nn.Conv1d(prev_size, size,1)\n",
        "            layers.append(conv)\n",
        "            layers.append(bn)\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_size = size\n",
        "        ########################################################################\n",
        "        return nn.Sequential(*layers)\n",
        "  \n",
        "    def _build_fc(self):\n",
        "        ########################################################################\n",
        "        # TODO:  the fully connected network that takes the feature space and \n",
        "        # learns the transformation matrix. \n",
        "        #   The hidden_layers according to hidden_sizes_fc\n",
        "        # \n",
        "        # Hint: the fully connected structur is as follows:\n",
        "        #   [Fully Connected Layer]-> [Batch Norm Layer] -> [ReLU]-> [Fully Connected Layer]-> ...\n",
        "        ########################################################################\n",
        "        layers = []\n",
        "        prev_size = self.hidden_sizes_conv[-1]\n",
        "        for layer_id, size in enumerate(self.hidden_sizes_fc):\n",
        "            bn = nn.BatchNorm1d(size)\n",
        "            fc = nn.Linear(prev_size, size)\n",
        "            layers.append(fc)\n",
        "            layers.append(bn)\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_size = size\n",
        "        layers.append(nn.Linear(self.hidden_sizes_fc[-1],self.k**2))\n",
        "        ########################################################################\n",
        "        return nn.Sequential(*layers)\n",
        "      \n",
        "\n",
        "    def forward(self, input):\n",
        "        ########################################################################\n",
        "        # TODO: Performs the forward pass of the T-Net. \n",
        "        # It first applies the convolutional network to the input point cloud \n",
        "        # to obtain a feature space. \n",
        "        # Then, it applies the fully connected network to the feature space to \n",
        "        # obtain the kxk transformation matrix. Finally, it applies the\n",
        "        # transformation matrix to the input point cloud to transform it to a \n",
        "        # canonical pose.\n",
        "        # \n",
        "        # Hint: the forward structure is as follows:\n",
        "        # [ConvLayers]->[MaxPooling]->[Flatten]->[Fully Connected Layers]->[theta_Matrix + identity]\n",
        "        #   The identity require gradient\n",
        "        ########################################################################\n",
        "        # input.shape (bs,n,3)\n",
        "        bs = input.size(0)\n",
        "        \n",
        "        xb = self.conv(input)   \n",
        "        pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        flat = nn.Flatten(1)(pool)\n",
        "        xb = self.fc(flat)\n",
        "      \n",
        "        init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "        if xb.is_cuda:\n",
        "          init=init.cuda()\n",
        "        matrix = xb.view(-1,self.k,self.k) + init        \n",
        "        return matrix"
      ],
      "metadata": {
        "id": "BbkCjFwWLyyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "test_t_net = Tnet()\n",
        "\n",
        "if count_parameters(test_t_net)!=803081:\n",
        "    print(\"Error\")\n",
        "    print(\"test_t_net parameters number = \", count_parameters(test_t_net))\n",
        "\n",
        "assert count_parameters(test_t_net)==803081"
      ],
      "metadata": {
        "id": "_0-ltVBEQKW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "x1 = torch.randn(3, 3, 5)\n",
        "print(x1)\n",
        "\n",
        "y1 = torch.tensor([[[ 1.4712e+00,  1.1447e+00,  6.5780e-02],\n",
        "         [ 2.6862e-01,  1.5355e+00, -7.9635e-01],\n",
        "         [-3.1744e-01,  4.8485e-01,  1.2669e+00]],\n",
        "\n",
        "        [[ 1.0652e+00, -2.9729e-02, -9.1289e-04],\n",
        "         [-2.0753e-01,  1.6646e+00,  5.0989e-01],\n",
        "         [-2.5312e-01,  7.1402e-01,  8.2575e-01]],\n",
        "\n",
        "        [[ 1.3445e+00,  6.7090e-01, -4.4554e-01],\n",
        "         [ 2.4452e-01,  1.1833e+00, -5.8614e-01],\n",
        "         [-5.3094e-02, -1.3413e-01,  9.4217e-01]]])\n",
        "pred_y1 = test_t_net(x1)\n",
        "print(pred_y1)\n",
        "assert torch.allclose(y1, pred_y1, rtol=1e-03, atol=1e-03),  \"different y_pred and y\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esqdzu8iL6mR",
        "outputId": "535d051c-17f9-482b-97bb-fa9ea30d4e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784],\n",
            "         [-1.2345, -0.0431, -1.6047, -0.7521,  1.6487],\n",
            "         [-0.3925, -1.4036, -0.7279, -0.5594, -0.7688]],\n",
            "\n",
            "        [[ 0.7624,  1.6423, -0.1596, -0.4974,  0.4396],\n",
            "         [-0.7581,  1.0783,  0.8008,  1.6806,  1.2791],\n",
            "         [ 1.2964,  0.6105,  1.3347, -0.2316,  0.6872]],\n",
            "\n",
            "        [[-1.0892, -0.3553, -0.9138, -0.6581,  0.0780],\n",
            "         [ 0.5258, -0.4880, -0.4345, -1.3864, -1.2862],\n",
            "         [-1.4032,  0.0360, -0.0635,  0.6756, -0.0978]]])\n",
            "tensor([[[ 1.4712e+00,  1.1447e+00,  6.5780e-02],\n",
            "         [ 2.6862e-01,  1.5355e+00, -7.9635e-01],\n",
            "         [-3.1744e-01,  4.8485e-01,  1.2669e+00]],\n",
            "\n",
            "        [[ 1.0652e+00, -2.9729e-02, -9.1289e-04],\n",
            "         [-2.0753e-01,  1.6646e+00,  5.0989e-01],\n",
            "         [-2.5312e-01,  7.1402e-01,  8.2575e-01]],\n",
            "\n",
            "        [[ 1.3445e+00,  6.7090e-01, -4.4554e-01],\n",
            "         [ 2.4452e-01,  1.1833e+00, -5.8614e-01],\n",
            "         [-5.3094e-02, -1.3413e-01,  9.4217e-01]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfrom Class\n",
        "The Transform class is every thing before last MLPs in PointNet. It is a neural network architecture that uses two pairs of spatial transform net (STN) and shared MLP layers to extract global features from a point cloud data of (nx3) shape. The STN is implemented using the T-Net and computes the 3x3 transform matrix, which is then multiplied with the input point cloud to get a transformed point cloud of the same shape. The transformed point cloud is then input into the shared MLP layers along with the feature transform matrix. The output from the shared MLP layers is max pooled along the feature dimension to get a global feature vector. The output also includes the point and feature transform matrices.\n",
        "![TransformNet](https://github.com/58191554/PointNet-Project/blob/main/img/PointNetStructureFromPaper.png?raw=true)"
      ],
      "metadata": {
        "id": "JU7YpR2wW4Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transform(nn.Module):\n",
        "    def __init__(self, input_size=3, feature_size=64, sharedMLP1_layers=[64, 64], sharedMLP2_layers=[64, 128, 1024], batch_norm = True):\n",
        "        \"\"\"\n",
        "        Transform class is all the pipeline to get a global feature\n",
        "                 _____________________                                     _______________                 ___________________       _______________\n",
        "                |                     |                                   |               |                |                 |     |               |                    \n",
        "        x -->   |   input transform   | --> y (canonical point cloud) --> |  shared MLP   | --> feature -->|feature transform| --> |  shared MLP   | --> max pooling --> z\n",
        "                |_____________________|                                   |_______________|                |_________________|     |  _____________|\n",
        "        The transform class is a neural networknet architecture that go throught 2 pairs of spactial transform net and shared MLP.\n",
        "        The STN is the T-Net that implement above, and the shared-MLP can be regarded as a one-dimensional convolutional layer.\n",
        "\n",
        "        the input x as a point cloud data of (nx3) shape first compute the 3x3 transform matrix and multiplied with the transform matrix to get a (nx3) transformed point cloud.\n",
        "\n",
        "        the last_activate bool is True when you want to add the last layer with activation function.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.batch_norm = True\n",
        "        \n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=64)\n",
        "\n",
        "        self.sharedMLP1 = self._build_sharedMLP(input_size, sharedMLP1_layers, last_activate=True)\n",
        "        self.sharedMLP2 = self._build_sharedMLP(feature_size, sharedMLP2_layers, last_activate=False)\n",
        "\n",
        "    def _build_sharedMLP(self, input_dim, sharedMLP_layers, last_activate = True):\n",
        "        ########################################################################\n",
        "        # TODO: Build the shared MLP layers \n",
        "        # Hint: \n",
        "        #   The structure is [Conv1d]->[Batch Norm]->[ReLU]\n",
        "        ########################################################################\n",
        "        layers = []\n",
        "        prev_size = input_dim\n",
        "        for layer_id, size in enumerate(sharedMLP_layers):\n",
        "            layers.append(nn.Conv1d(prev_size, size, 1))\n",
        "\n",
        "            if self.batch_norm:\n",
        "                layers.append(nn.BatchNorm1d(size))\n",
        "\n",
        "            if (layer_id < len(sharedMLP_layers)-1) or last_activate:\n",
        "                layers.append(nn.ReLU())\n",
        "\n",
        "            prev_size = size\n",
        "        return nn.Sequential(*layers)\n",
        "       \n",
        "    def forward(self, input):     #input:[batch_size, 3, 1024] output:[batch_size, 1024]\n",
        "    \n",
        "        ########################################################################\n",
        "        # TODO: Implement the code to multiply the transform matrix and the point\n",
        "        # cloud. The transformed x should be the same shape of x \n",
        "        # \n",
        "        # Hint: \n",
        "        # 1. Get the transform matrix by the T-Net\n",
        "        # 2. Batch matrix multiply the input x and transform matrix\n",
        "        # 3. Input the data into the Shared MLP\n",
        "        # 4. Batch matrix multiply the feature and the feature_transform matrix\n",
        "        # 5. Input the output into the Shared MLP with feature dimension\n",
        "        # 6. Maxpooling along the feature dimension\n",
        "        # 7. output the output data, points transform matrix, and the feature\n",
        "        #       transform matrix\n",
        "        ########################################################################\n",
        "        matrix3x3 = self.input_transform(input)     #[batch_size, 3, 3]\n",
        "        # batch matrix multiplication\n",
        "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)     #[batch_size, 3, 1024]\n",
        "        xb = self.sharedMLP1(xb)\n",
        "\n",
        "        matrix64x64 = self.feature_transform(xb)     #[batch_size, 64, 64]\n",
        "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)     #[batch_size, 64, 1024]\n",
        "        xb = self.sharedMLP2(xb)\n",
        "\n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)     #[batch_size, 1024, 1]\n",
        "        output = nn.Flatten(1)(xb)     #[batch_size, 1024]\n",
        "        ########################################################################\n",
        "        return output, matrix3x3, matrix64x64"
      ],
      "metadata": {
        "id": "HkMoW3AySa4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "test_transfrom_net = Transform()\n",
        "\n",
        "test_tranform_net_param_num = count_parameters(test_transfrom_net)\n",
        "if test_tranform_net_param_num!=2812105:\n",
        "    print(\"Error\")\n",
        "    print(\"test_transfrom_net parameters number = \", count_parameters(test_transfrom_net))\n",
        "    print(\"Difference = \", torch.absolute(test_tranform_net_param_num!=2812105))\n",
        "\n",
        "assert count_parameters(test_transfrom_net)==2812105\n",
        "\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "x2 = torch.randn(2, 3, 5)\n",
        "pred_y2, pred_mat1, pred_mat2 = test_transfrom_net(x2)\n",
        "\n",
        "mat1 = torch.tensor([[[ 1.0655,  0.4169,  0.1452],\n",
        "         [ 0.0240,  1.7885, -0.6011],\n",
        "         [-0.5582,  0.6857,  1.0256]],\n",
        "\n",
        "        [[ 1.5976,  0.8703, -0.4317],\n",
        "         [ 0.2260,  1.2361,  0.0457],\n",
        "         [ 0.0986,  0.0844,  1.0267]]])\n",
        "\n",
        "if not torch.allclose(mat1, pred_mat1, rtol=1e-03, atol=1e-03):\n",
        "    print(\"Error\")\n",
        "    print(\"The answer mat1 is \\n\", mat1)\n",
        "    print(\"The pred_mat1 is \\n\", pred_mat1)\n",
        "    print(\"Difference = \", torch.norm(pred_mat1- mat1))\n",
        "assert torch.allclose(mat1, pred_mat1, rtol=1e-03, atol=1e-03),  \"different pred_mat1 and mat1\""
      ],
      "metadata": {
        "id": "nr2gIhOOSc37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PointNet Classifier\n",
        "The code defines a PyTorch module called PointNet for classifying point cloud data. The PointNet module includes a Transform class, which takes in 3D point cloud data as input and generates global features and transformation matrices. The global features are then passed through a multi-layer perceptron (MLP) to generate scores for classification. The MLP consists of linear layers, batch normalization, ReLU activation, and dropout layers. The PointNet module outputs the logsoftmax of the scores along with the 3x3 and 64x64 transformation matrices generated by the Transform class. The PointNet module can be customized with different layer configurations, batch normalization, and dropout rates."
      ],
      "metadata": {
        "id": "WD88J6ysXTy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointNet(nn.Module):\n",
        "    def __init__(self, sharedMLP1_layers=[64, 64], sharedMLP2_layers=[64, 128, 1024], classes = 10, batch_norm = True, dropout_rate = 0.3):\n",
        "        \"\"\"\n",
        "        Point Net the whole neural network for the classification of point cloud data\n",
        "                    _________                         ___     \n",
        "        input x--->|Transform|---> global feature--->|MLP|---> scores\n",
        "                   |_________|                       |___|\n",
        "            args:   sharedMLP1_layers is the first shared MLP in Transform class\n",
        "                    sharedMLP2_layers is the second shared MLP in Transorm class\n",
        "                The MLP has the structure of \n",
        "                    [Linear] -> [Batch Norm] -> [ReLU] -> [Dropout] -> [Linear] -> ... -> [Linear] -> [Batch Norm] -> [ReLU] -> [Linear of class size]\n",
        "\n",
        "                \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.transform = Transform(input_size=3, feature_size=64, sharedMLP1_layers=sharedMLP1_layers, sharedMLP2_layers=sharedMLP2_layers)\n",
        "        self.batch_norm = batch_norm\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, classes)\n",
        "        \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def _build_fc(self, input_dim, fc_layers, cls_num, dropout_rate):\n",
        "        layers = []\n",
        "        prev_size = input_dim\n",
        "        for layer_id, size in enumerate(fc_layers):\n",
        "            layers.append(nn.Linear(prev_size, size, 1))\n",
        "\n",
        "            if self.batch_norm:\n",
        "                layers.append(nn.BatchNorm1d(size))\n",
        "\n",
        "            if layer_id < len(fc_layers):\n",
        "                layers.append(nn.ReLU())\n",
        "\n",
        "            if layer_id < len(fc_layers)-1:\n",
        "                layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_size = size\n",
        "        \n",
        "        layers.append(nn.Linear(cls_num))\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        ########################################################################\n",
        "        # TODO: get the output y, 3x3 transform matrix and 64x64 transform\n",
        "        # matrix from self.transform net.\n",
        "        # Then, y->[fc1]->[bn1]->[relu]->[fc2]->[dropout]->[bn2]->[relu]->[fc3]->z\n",
        "        # return logsoftmax(z), 3x3 transform matrix and 64x64 transform matrix\n",
        "        ########################################################################\n",
        "\n",
        "        xb, matrix3x3, matrix64x64 = self.transform(input)\n",
        "        xb = F.relu(self.bn1(self.fc1(xb)))\n",
        "        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n",
        "        output = self.fc3(xb)\n",
        "        return self.logsoftmax(output), matrix3x3, matrix64x64"
      ],
      "metadata": {
        "id": "25B0kuTRSm2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "test_point_net = PointNet()\n",
        "\n",
        "test_point_net_param_num = count_parameters(test_point_net)\n",
        "if test_point_net_param_num!=3472339:\n",
        "    print(\"Error\")\n",
        "    print(\"test_transfrom_net parameters number = \", count_parameters(test_point_net))\n",
        "    print(\"Difference = \", torch.absolute(torch.tensor(test_point_net_param_num-3472339)))\n",
        "\n",
        "assert count_parameters(test_point_net)==3472339\n",
        "\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "x3 = torch.randn(3, 3, 5)\n",
        "w, pred_mat3x3, pred_matfxf = test_point_net(x3)\n",
        "\n",
        "mat3x3 = torch.tensor([[[ 1.4712e+00,  1.1447e+00,  6.5780e-02],\n",
        "         [ 2.6862e-01,  1.5355e+00, -7.9635e-01],\n",
        "         [-3.1744e-01,  4.8485e-01,  1.2669e+00]],\n",
        "\n",
        "        [[ 1.0652e+00, -2.9729e-02, -9.1289e-04],\n",
        "         [-2.0753e-01,  1.6646e+00,  5.0989e-01],\n",
        "         [-2.5312e-01,  7.1402e-01,  8.2575e-01]],\n",
        "\n",
        "        [[ 1.3445e+00,  6.7090e-01, -4.4554e-01],\n",
        "         [ 2.4452e-01,  1.1833e+00, -5.8614e-01],\n",
        "         [-5.3094e-02, -1.3413e-01,  9.4217e-01]]])\n",
        "if not torch.allclose(mat3x3, pred_mat3x3, rtol=1e-03, atol=1e-03):\n",
        "    print(\"Error\")\n",
        "    print(\"The answer mat3x3 is \\n\", mat1)\n",
        "    print(\"The pred_mat3x3 is \\n\", pred_mat1)\n",
        "    print(\"Difference = \", torch.norm(pred_mat3x3- mat3x3))\n",
        "assert torch.allclose(pred_mat3x3, mat3x3, rtol=1e-03, atol=1e-03),  \"different pred_mat1 and mat1\""
      ],
      "metadata": {
        "id": "Pcg3dDW9ZdZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}