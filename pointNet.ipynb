{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D5RcOnwQaJB6"
      ],
      "mount_file_id": "1khf0iVhV0Dm32oEo7kE-VsjonAWqRZT0",
      "authorship_tag": "ABX9TyP+KY04nhyBLjrAtdLV6T5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/58191554/PointNet-Project/blob/TongZhen-branch/pointNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch matplotlib numpy os\n",
        "import sys\n",
        "import os\n",
        "print(\"\\nPython version\")\n",
        "print(sys.version)\n",
        "print(\"Python version info\")\n",
        "print(sys.version_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvXcsqFTybfu",
        "outputId": "eae72870-3e75-4736-a670-bd6e59d410d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Python version\n",
            "3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "Python version info\n",
            "sys.version_info(major=3, minor=9, micro=16, releaselevel='final', serial=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcYpcSLCLjQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d049d30-5a80-46cf-c9ce-2b1ff242e09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ModelNet10 Exists\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_folder = \"/content/drive/MyDrive/PointNet/\"\n",
        "\n",
        "os.makedirs(root_folder, exist_ok=True)\n",
        "os.chdir(root_folder)\n",
        "\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "\n",
        "# if not os.path.exists(\"ModelNet10.zip\"):\n",
        "#     print(\"Down Loading ModelNet10.zip ...\")\n",
        "#     urllib.request.urlretrieve(\"https://vision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\", \"ModelNet10.zip\")\n",
        "#     print(\"Finish DownLoading！\")\n",
        "\n",
        "# if not os.path.exists(\"ModelNet10\"):\n",
        "#     print(\"unzipping ModelNet10.zip...\")\n",
        "#     with zipfile.ZipFile(\"ModelNet10.zip\", \"r\") as zip_ref:\n",
        "#         zip_ref.extractall(\".\")\n",
        "#     print(\"Finish unzipping\")\n",
        "# else:\n",
        "#     print(\"ModelNet10 Exists\")\n",
        "\n",
        "# def remove_ds_store(folder_path):\n",
        "#     for root, dirs, files in os.walk(folder_path):\n",
        "#         for file in files:\n",
        "#             if file == \".DS_Store\":\n",
        "#                 file_path = os.path.join(root, file)\n",
        "#                 os.remove(file_path)\n",
        "#                 print(\"Deleted:\", file_path)\n",
        "\n",
        "# remove_ds_store(root_folder+\"ModelNet10\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "_bZvDi8bOecm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_str_list = [\n",
        "    \"bathtub\",\n",
        "    \"bed\",\n",
        "    \"chair\",\n",
        "    \"desk\",\n",
        "    \"dresser\",\n",
        "    \"monitor\",\n",
        "    \"night_stand\",\n",
        "    \"sofa\",\n",
        "    \"table\",\n",
        "    \"toilet\"\n",
        "]\n",
        "\n",
        "class_to_idx = {}\n",
        "idx_to_class = {}\n",
        "for idx, class_str in enumerate(class_str_list):\n",
        "  class_to_idx[class_str] = idx\n",
        "  idx_to_class[idx] = class_str\n",
        "\n",
        "for key, value in class_to_idx.items():\n",
        "    print(f\"key：{key}, value：{value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNfi_Rp5EciK",
        "outputId": "95789dd3-484e-4cdf-c6fb-c37db71acbc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key：bathtub, value：0\n",
            "key：bed, value：1\n",
            "key：chair, value：2\n",
            "key：desk, value：3\n",
            "key：dresser, value：4\n",
            "key：monitor, value：5\n",
            "key：night_stand, value：6\n",
            "key：sofa, value：7\n",
            "key：table, value：8\n",
            "key：toilet, value：9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Class"
      ],
      "metadata": {
        "id": "D5RcOnwQaJB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointDataSet(torch.utils.data.Dataset):\n",
        "  def __init__(self, root_dir, k=500):\n",
        "    self.root_dir = root_dir\n",
        "    self.k = k\n",
        "    self.items = []      #N\n",
        "    self.classes = []     #[0, 1, 2,...,10]\n",
        "    self.class_to_idx = class_to_idx\n",
        "    self.idx_to_class = idx_to_class\n",
        "    self.data = []       #NxMx3\n",
        "    self.label = []      #N\n",
        "    self.y = None       #Nx10\n",
        "\n",
        "    folders = os.listdir(root_dir)\n",
        "        \n",
        "    # get different objects\n",
        "    for idx, item_folder in idx_to_class.items():\n",
        "\n",
        "      if idx > 2:break\n",
        "      print(idx, item_folder)\n",
        "      if not os.path.isdir(os.path.join(root_dir, item_folder)):\n",
        "        continue\n",
        "            \n",
        "      # record class\n",
        "      self.classes.append(item_folder)\n",
        "      self.class_to_idx[item_folder] = idx\n",
        "      self.idx_to_class[idx] = item_folder\n",
        "            \n",
        "      # get train data\n",
        "      train_folder = os.path.join(root_dir, item_folder, 'train')\n",
        "      if os.path.exists(train_folder):\n",
        "        train_files = [f for f in os.listdir(train_folder) if f.endswith('.off')]\n",
        "        self.items.extend([(os.path.join(train_folder, f), idx) for f in train_files])\n",
        "        self.label.extend([idx for f in train_files])\n",
        "\n",
        "      # get test data\n",
        "      test_folder = os.path.join(root_dir, item_folder, 'test')\n",
        "      if os.path.exists(test_folder):\n",
        "        test_files = [f for f in os.listdir(test_folder) if f.endswith('.off')]\n",
        "        self.items.extend([(os.path.join(test_folder, f), idx) for f in test_files])\n",
        "        self.label.extend([idx for f in test_files])\n",
        "\n",
        "      # break\n",
        "\n",
        "    # get tensor data\n",
        "    # self.load_to_RAM()\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    y = torch.zeros((1, len(self.classes)))\n",
        "    y[0, index]=1\n",
        "    points = self.read_off_file(self.items[index][0]), y\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.items)\n",
        "\n",
        "  def load_to_RAM(self):\n",
        "    # reduce I/O, read into train/test_data list of torch tensor\n",
        "    print(\"Load classes: \", self.classes)\n",
        "    self.y = torch.zeros((len(self.items),len(self.classes)))\n",
        "    for i, (f, c) in tqdm(enumerate(self.items), dynamic_ncols=True, leave=True):\n",
        "      verts, faces = self.read_off_file(f)\n",
        "      self.y[i,c] = 1 \n",
        "    \n",
        "  def read_off_file(self, file_path):\n",
        "    # return numpy\n",
        "    with open(file_path, 'r') as file:\n",
        "      if 'OFF' != file.readline().strip():\n",
        "        raise('Not a valid OFF header')\n",
        "      n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
        "      verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
        "      print(\"fucck\")\n",
        "      faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
        "      print(\"ass hole\")\n",
        "      points = np.float32(self.sample(verts, faces, self.k))\n",
        "      \n",
        "      # self.data.append(torch.from_numpy(points))\n",
        "\n",
        "    return points\n",
        "\n",
        "  def triangle_area(self, pt1, pt2, pt3):\n",
        "    # compute the area of the triangle by Heron's formula\n",
        "    side_a = np.linalg.norm(pt1 - pt2)\n",
        "    side_b = np.linalg.norm(pt2 - pt3)\n",
        "    side_c = np.linalg.norm(pt3 - pt1)\n",
        "    s = 0.5 * (side_a + side_b + side_c)\n",
        "    return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5 \n",
        "\n",
        "  def sample_point(self, pt1, pt2, pt3):\n",
        "    # barycentric coordinates on a triangle\n",
        "    s, t = sorted([random.random(), random.random()])\n",
        "    f = lambda i: s * pt1[i] + (t-s) * pt2[i] + (1-t) * pt3[i]\n",
        "    return [f(0), f(1), f(2)]\n",
        "\n",
        "  def sample(self, verts, faces, k):\n",
        "    # k: number of samples\n",
        "    # return a kx3 matrix of point clouds\n",
        "    areas = np.zeros((len(faces)))\n",
        "    verts = np.array(verts)\n",
        "    for i in range(len(areas)):\n",
        "      areas[i] = self.triangle_area(verts[faces[i][0]], verts[faces[i][1]], verts[faces[i][2]])\n",
        "      # sample k points from each face with weights by their areas\n",
        "      sampled_faces = (random.choices(faces, weights=areas, k=k))\n",
        "      pointcloud = np.zeros((k, 3))\n",
        "\n",
        "      for i in range(len(sampled_faces)):\n",
        "        pointcloud[i] = self.sample_point(verts[sampled_faces[i][0]],\n",
        "                          verts[sampled_faces[i][1]],\n",
        "                          verts[sampled_faces[i][2]])\n",
        "    print(\"shit\")\n",
        "    return pointcloud\n",
        "\n",
        "\n",
        "  def get_min_points_num(self):\n",
        "    # return the minimum number of points among all the objects.\n",
        "    min_num = self[0][0].size(0)\n",
        "    for ts in self.data:\n",
        "      if(min_num > ts.size(0)):\n",
        "        min_num = ts.size(0)\n",
        "    return min_num\n",
        "\n",
        "  def get_unify_data(self):\n",
        "    min_num = self.get_min_points_num()\n",
        "    unify_data = []\n",
        "    unify_data.extend([random_sample(points, min_num) for points in self.data])\n",
        "    return unify_data\n",
        "\n",
        "def plot3d(points, plot_line = False):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  # get xyz coordinate data\n",
        "  x = points[:, 0]\n",
        "  y = points[:, 1]\n",
        "  z = points[:, 2]\n",
        "\n",
        "  # plot vertices\n",
        "  ax.scatter(x, y, z, c='b', marker='o', s=1)  # c: 点的颜色，marker: 点的形状，s: 点的大小\n",
        "\n",
        "  # plot line\n",
        "  if plot_line:\n",
        "    for i in range(points.shape[0]):\n",
        "      ax.plot([x[i], x[(i + 1) % points.shape[0]]],\n",
        "          [y[i], y[(i + 1) % points.shape[0]]],\n",
        "          [z[i], z[(i + 1) % points.shape[0]]], c='r', linewidth=0.5)\n",
        "\n",
        "  # set axis label\n",
        "  ax.set_xlabel('X')\n",
        "  ax.set_ylabel('Y')\n",
        "  ax.set_zlabel('Z')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def random_sample(points, num):\n",
        "  if num > points.size(0):\n",
        "    return []\n",
        "  indices = torch.randperm(points.size(0))[:num]\n",
        "  # 从输入的 torch tensor 中根据索引选择对应的向量\n",
        "  selected_vectors = [points[i] for i in indices]\n",
        "\n",
        "  sample = torch.cat(selected_vectors, dim=0)\n",
        "  return sample"
      ],
      "metadata": {
        "id": "mVxXUZhF0ta-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Class 2"
      ],
      "metadata": {
        "id": "iHtOxmULaZ6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelNetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 npoints=2500,\n",
        "                 split='train',\n",
        "                 data_augmentation=True):\n",
        "        self.npoints = npoints\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.fns = []\n",
        "        with open(os.path.join(root, '{}.txt'.format(self.split)), 'r') as f:\n",
        "            for line in f:\n",
        "                self.fns.append(line.strip())\n",
        "\n",
        "        self.cat = {}\n",
        "        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../misc/modelnet_id.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                ls = line.strip().split()\n",
        "                self.cat[ls[0]] = int(ls[1])\n",
        "\n",
        "        print(self.cat)\n",
        "        self.classes = list(self.cat.keys())\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fn = self.fns[index]\n",
        "        cls = self.cat[fn.split('/')[0]]\n",
        "        with open(os.path.join(self.root, fn), 'rb') as f:\n",
        "            plydata = PlyData.read(f)\n",
        "        pts = np.vstack([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T\n",
        "        choice = np.random.choice(len(pts), self.npoints, replace=True)\n",
        "        point_set = pts[choice, :]\n",
        "\n",
        "        point_set = point_set - np.expand_dims(np.mean(point_set, axis=0), 0)  # center\n",
        "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis=1)), 0)\n",
        "        point_set = point_set / dist  # scale\n",
        "\n",
        "        if self.data_augmentation:\n",
        "            theta = np.random.uniform(0, np.pi * 2)\n",
        "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "            point_set[:, [0, 2]] = point_set[:, [0, 2]].dot(rotation_matrix)  # random rotation\n",
        "            point_set += np.random.normal(0, 0.02, size=point_set.shape)  # random jitter\n",
        "\n",
        "        point_set = torch.from_numpy(point_set.astype(np.float32))\n",
        "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
        "        return point_set, cls\n"
      ],
      "metadata": {
        "id": "O3ysgfBIaYiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "2uV6b2BUaSi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/PointNet/ModelNet10\"\n",
        "pointData = PointDataSet(data_path)\n",
        "points, label = pointData[2]\n",
        "plot3d(points)"
      ],
      "metadata": {
        "id": "3gs2fZa5VDgg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "5d46f775-e1c5-43b8-a896-a623f1b1aaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 bathtub\n",
            "1 bed\n",
            "2 chair\n",
            "fucck\n",
            "ass hole\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-57b396e0fae3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/PointNet/ModelNet10\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpointData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPointDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpointData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplot3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-df43d8ef9e06>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_off_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-df43d8ef9e06>\u001b[0m in \u001b[0;36mread_off_file\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_face\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_faces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ass hole\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;31m# self.data.append(torch.from_numpy(points))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-df43d8ef9e06>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, verts, faces, k)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0;31m# sample k points from each face with weights by their areas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0msampled_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mareas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m       \u001b[0mpointcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_faces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pointData)):\n",
        "  print(pointData[i][1])"
      ],
      "metadata": {
        "id": "BMucphvVHCwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成规定长度的数据集，用于MLP\n",
        "class UnifyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "        \n",
        "  def __getitem__(self, index):\n",
        "    x_item = self.x[index]\n",
        "    y_item = self.y[index]\n",
        "    return x_item, y_item\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.x)"
      ],
      "metadata": {
        "id": "s-J-7Ql-Kfxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try MLP"
      ],
      "metadata": {
        "id": "2k8mv2-dZTRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "batch_size = 16\n",
        "\n",
        "train_size = int(train_ratio * len(pointData))\n",
        "test_size = len(pointData) - train_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(pointData, [train_size, test_size])\n",
        "\n",
        "# 创建训练数据和测试数据\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "uniData = UnifyDataset(pointData.get_unify_data(), pointData.label)\n",
        "print(len(uniData))\n",
        "train_unify_dataset, test_unify_dataset= torch.utils.data.random_split(uniData, [train_size, test_size])\n",
        "train_unify_dataloader = torch.utils.data.DataLoader(train_unify_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_unify_dataloader = torch.utils.data.DataLoader(test_unify_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "i5DNOhREEAaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, layers):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layers = nn.ModuleList()  # 使用 ModuleList 存储每一层的 Module\n",
        "    for i in range(1, len(layers)):\n",
        "      self.layers.append(nn.Linear(layers[i-1], layers[i]))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = torch.relu(layer(x))\n",
        "      x = torch.softmax(x, dim=0)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Z79F0tG-ZQMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train MLP "
      ],
      "metadata": {
        "id": "fpwii7MYb_Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_num_point = pointData.get_min_points_num()\n",
        "print(min_num_point)\n",
        "layers = [min_num_point * 3, 64, 32, 10]  # MLP 模型的每层维度\n",
        "model = MLP(layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 模型训练\n",
        "num_epochs = 100  # 设定训练的轮数\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 检查是否有可用的 GPU\n",
        "model.to(device)  # 将模型移动到对应的设备\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()  # 将模型设置为训练模式\n",
        "  for i, (inputs, targets) in enumerate(train_unify_dataloader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)  # 将输入和标签移动到对应的设备\n",
        "    optimizer.zero_grad()  # 清零优化器梯度\n",
        "        \n",
        "    # 前向传播和计算损失\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "        \n",
        "    # 反向传播和优化\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 打印训练信息\n",
        "  if epoch % 10 == 0:\n",
        "    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, len(train_unify_dataloader), loss.item()))\n"
      ],
      "metadata": {
        "id": "qjK_PD3xcBDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LWqk7_MQarwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KYOeKHXqOk0E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}